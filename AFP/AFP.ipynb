{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "from models import Model_TGCN, batch_size\n",
    "from data_pretreatment import func\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import dgl\n",
    "from dgllife.utils import *\n",
    "from dgllife.model.model_zoo.gcn_predictor import GCNPredictor\n",
    "torch.cuda.empty_cache()\n",
    "import numpy as np\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from dgllife.utils import smiles_to_bigraph\n",
    "from dgllife.utils import EarlyStopping, Meter\n",
    "from dgllife.utils import AttentiveFPAtomFeaturizer\n",
    "from dgllife.utils import AttentiveFPBondFeaturizer\n",
    "from dgllife.utils import SMILESToBigraph, ScaffoldSplitter, RandomSplitter\n",
    "from functools import partial\n",
    "from dgllife.model.model_zoo.attentivefp_predictor import AttentiveFPPredictor\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score, matthews_corrcoef, recall_score, f1_score\n",
    "import random\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "mol = Chem.MolFromSmiles('CNC(=O)/C=C/c1cnc2ccc(OC(C)c3c(Cl)ccc(F)c3Cl)cc2c1')\n",
    "atom_featurizer = AttentiveFPAtomFeaturizer(atom_data_field='hv')\n",
    "bond_featurizer = AttentiveFPBondFeaturizer(bond_data_field='he')\n",
    "n_feats = atom_featurizer.feat_size('hv')\n",
    "e_feats = bond_featurizer.feat_size('he')\n",
    "print(n_feats)\n",
    "print(e_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_to_g = SMILESToBigraph(node_featurizer=atom_featurizer, edge_featurizer=bond_featurizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df):\n",
    "    g = [smiles_to_g(s) for s in df['SMILES']]\n",
    "    y = np.array(list((df['label'])))\n",
    "    y = np.array(y, dtype=np.int64)\n",
    "    return g, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch,train_d):\n",
    "    gcn_net.train()\n",
    "    y_l=[]\n",
    "    y_p=[]\n",
    "    train_epoch_loss, train_epoch_acc, train_epoch_r2 = 0, 0, 0\n",
    "    for i, (X, list_num, graph, labels, index) in enumerate(train_d):\n",
    "        train_labels = labels.to(device)\n",
    "        n_feats = graph.ndata.pop('hv').to(device)\n",
    "        e_feats = graph.edata.pop('he').to(device)\n",
    "        #print(n_feats)\n",
    "        #print(e_feats)\n",
    "        n_feats, e_feats, train_labels = n_feats.to(device), e_feats.to(device), train_labels.to(device)\n",
    "        graph = graph.to(device)\n",
    "        y_ = gcn_net(graph, n_feats, e_feats).to('cpu')\n",
    "        y = torch.reshape(y_, [batch_size])\n",
    "        train_loss = nn.BCELoss()(y, train_labels.float().to('cpu'))\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.requires_grad_(True)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_epoch_loss += train_loss.detach().item()\n",
    "        train_true_label = train_labels.to('cpu').numpy()\n",
    "        yy = [1 if i >= 0.5 else 0 for i in y.detach().numpy()]\n",
    "        train_epoch_acc += sum(train_true_label == yy)\n",
    "        y_l.extend(train_labels.cpu().tolist())\n",
    "        y_p.extend(y_.cpu().tolist())\n",
    "    train_epoch_acc = train_epoch_acc / train_true_label.shape[0]\n",
    "    train_epoch_acc /= (i + 1)\n",
    "    train_epoch_loss /= (i + 1)\n",
    "    auc = roc_auc_score(y_l, y_p)\n",
    "    y_p_binary = [1 if p > 0.5 else 0 for p in np.array(y_p)]\n",
    "    mcc = matthews_corrcoef(y_l, y_p_binary)\n",
    "    y_l_np = np.array(y_l)\n",
    "    y_p_binary_np = np.array(y_p_binary)\n",
    "    recall_positive = recall_score(y_l_np, y_p_binary_np, pos_label=1)\n",
    "    recall_negative = recall_score(y_l_np, y_p_binary_np, pos_label=0)\n",
    "    ba = (recall_positive + recall_negative) / 2\n",
    "    f1 = f1_score(y_l_np, y_p_binary_np)\n",
    "    return train_epoch_acc,train_epoch_loss,auc, mcc,ba,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch,test_d):\n",
    "    epoch_loss, epoch_acc = 0, 0\n",
    "    mlist = []\n",
    "    gcn_net.eval()\n",
    "    y_l=[]\n",
    "    y_p=[]\n",
    "    for i, (X, list_num, graph, labels, index) in enumerate(test_d):\n",
    "        labels = labels.to(device)\n",
    "        n_feats = graph.ndata.pop('hv').to(device)\n",
    "        e_feats = graph.edata.pop('he').to(device)\n",
    "        #print(n_feats)\n",
    "        #print(e_feats)\n",
    "        n_feats, e_feats, labels = n_feats.to(device), e_feats.to(device), labels.to(device)\n",
    "        graph = graph.to(device)\n",
    "        y_ = gcn_net(graph, n_feats, e_feats).to('cpu')\n",
    "        y = torch.reshape(y_, [test_d.batch_size])\n",
    "        loss = nn.BCELoss()(y, labels.float().to('cpu'))\n",
    "        epoch_loss += loss.detach().item()\n",
    "        pred_cls = y.detach().numpy()\n",
    "        true_label = labels.to('cpu').numpy()\n",
    "        yy = [1 if m >= 0.5 else 0 for m in y.detach().numpy()]\n",
    "        mlist.extend(pred_cls)\n",
    "        epoch_acc += sum(true_label == yy)\n",
    "        y_l.extend(labels.cpu().tolist())\n",
    "        y_p.extend(y_.cpu().tolist())\n",
    "    auc = roc_auc_score(y_l, y_p)\n",
    "    y_p_binary = [1 if p > 0.5 else 0 for p in np.array(y_p)]\n",
    "    mcc = matthews_corrcoef(y_l, y_p_binary)\n",
    "    epoch_acc = epoch_acc / true_label.shape[0]\n",
    "    epoch_acc /= (i + 1)\n",
    "    epoch_loss /= (i + 1)\n",
    "    y_l_np = np.array(y_l)\n",
    "    y_p_binary_np = np.array(y_p_binary)\n",
    "    recall_positive = recall_score(y_l_np, y_p_binary_np, pos_label=1)\n",
    "    recall_negative = recall_score(y_l_np, y_p_binary_np, pos_label=0)\n",
    "    ba = (recall_positive + recall_negative) / 2\n",
    "    f1 = f1_score(y_l_np, y_p_binary_np)\n",
    "\n",
    "    return epoch_acc, epoch_loss, auc, mcc,ba,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(sample):\n",
    "    _, list_num, graphs, labels, index = map(list, zip(*sample))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    batched_graph.set_n_initializer(dgl.init.zero_initializer)\n",
    "    batched_graph.set_e_initializer(dgl.init.zero_initializer)\n",
    "    return _, list_num, batched_graph, torch.tensor(labels), index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(test_d):\n",
    "    epoch_loss, epoch_acc = 0, 0\n",
    "    mlist = []\n",
    "    gcn_net.eval()\n",
    "    y_l=[]\n",
    "    y_p=[]\n",
    "    for i, (X, list_num, graph, labels, index) in enumerate(test_d):\n",
    "        labels = labels.to(device)\n",
    "        n_feats = graph.ndata.pop('hv').to(device)\n",
    "        e_feats = graph.edata.pop('he').to(device)\n",
    "        #print(n_feats)\n",
    "        #print(e_feats)\n",
    "        n_feats, e_feats, labels = n_feats.to(device), e_feats.to(device), labels.to(device)\n",
    "        graph = graph.to(device)\n",
    "        y_ = gcn_net(graph, n_feats, e_feats).to('cpu')\n",
    "        y = torch.reshape(y_, [test_d.batch_size])\n",
    "        loss = nn.BCELoss()(y, labels.float().to('cpu'))\n",
    "        epoch_loss += loss.detach().item()\n",
    "        pred_cls = y.detach().numpy()\n",
    "        true_label = labels.to('cpu').numpy()\n",
    "        yy = [1 if m >= 0.5 else 0 for m in y.detach().numpy()]\n",
    "        mlist.extend(pred_cls)\n",
    "        epoch_acc += sum(true_label == yy)\n",
    "        y_l.extend(labels.cpu().tolist())\n",
    "        y_p.extend(y_.cpu().tolist())\n",
    "    auc = roc_auc_score(y_l, y_p)\n",
    "    y_p_binary = [1 if p > 0.5 else 0 for p in np.array(y_p)]\n",
    "    mcc = matthews_corrcoef(y_l, y_p_binary)\n",
    "    epoch_acc = epoch_acc / true_label.shape[0]\n",
    "    epoch_acc /= (i + 1)\n",
    "    epoch_loss /= (i + 1)\n",
    "    y_l_np = np.array(y_l)\n",
    "    y_p_binary_np = np.array(y_p_binary)\n",
    "    recall_positive = recall_score(y_l_np, y_p_binary_np, pos_label=1)\n",
    "    recall_negative = recall_score(y_l_np, y_p_binary_np, pos_label=0)\n",
    "    ba = (recall_positive + recall_negative) / 2\n",
    "    f1 = f1_score(y_l_np, y_p_binary_np)\n",
    "\n",
    "    return epoch_acc, epoch_loss, auc, mcc,ba,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses = []\n",
    "val_acc = []\n",
    "val_auc = []\n",
    "val_mcc = []\n",
    "val_ba = []\n",
    "val_f1 = []\n",
    "test_losses1 = []\n",
    "test_acc1 = []\n",
    "test_auc1 = []\n",
    "test_mcc1 = []\n",
    "test_ba1 = []\n",
    "test_f11 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    best_param ={}\n",
    "    best_param[\"roc_epoch\"] = 0\n",
    "    best_param[\"mcc_epoch\"] = 0\n",
    "    best_param[\"loss_epoch\"] = 0\n",
    "    best_param[\"score_epoch\"] = 0\n",
    "    best_param[\"valid_roc\"] = 0\n",
    "    best_param[\"valid_mcc\"] = 0\n",
    "    best_param[\"valid_score\"] = 0\n",
    "    best_param[\"valid_loss\"] = 9e8\n",
    "    print('Split '+str(i+1)+' ......')\n",
    "    PATH_x_train = 'X_train{}.csv'.format(i+1)\n",
    "    PATH_x_val = 'X_val{}.csv'.format(i+1)\n",
    "    #PATH_x_test = 'X_test{}.csv'.format(i+1)\n",
    "    \n",
    "    \n",
    "    df_seq_train, y_train_tensor, y_true_train, list_num_train = func(PATH_x_train)\n",
    "    df_seq_val, y_val_tensor, y_true_val, list_num_val = func(PATH_x_val)\n",
    "    #df_seq_test, y_test_tensor, y_true_test, list_num_test = func(PATH_x_test)\n",
    "    \n",
    "    train_X = pd.read_csv(PATH_x_train)\n",
    "    x_train, y_train = get_data(train_X)\n",
    "    train_data = list(zip(df_seq_train, list_num_train, x_train, y_train, [i for i in range(len(train_X))]))\n",
    "    train_loader_ = DataLoader(train_data, batch_size=batch_size, shuffle=False, collate_fn=collate, drop_last=True)\n",
    "   \n",
    "    val_X = pd.read_csv(PATH_x_val)\n",
    "    x_val, y_val = get_data(val_X)\n",
    "    val_data = list(zip(df_seq_val, list_num_val, x_val, y_val, [i for i in range(len(val_X))]))\n",
    "    val_loader_val = DataLoader(val_data, batch_size=90, shuffle=False, collate_fn=collate, drop_last=True)\n",
    "    \n",
    "    #test_X = pd.read_csv(PATH_x_test)\n",
    "    #x_test, y_test = get_data(test_X)\n",
    "    #test_data = list(zip(df_seq_test, list_num_test, x_test, y_test, [i for i in range(len(test_X))]))\n",
    "    #test_loader_test = DataLoader(test_data, batch_size=1, shuffle=False, collate_fn=collate, drop_last=True)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gcn_net = AttentiveFPPredictor(node_feat_size=n_feats,\n",
    "                                       edge_feat_size=e_feats,\n",
    "                                       num_layers=2,\n",
    "                                       num_timesteps=1,\n",
    "                                       graph_feat_size=100,\n",
    "                                       n_tasks=1,\n",
    "                                       dropout=0.4\n",
    "                                        )\n",
    "    gcn_net = gcn_net.to(\"cuda\")\n",
    "    \n",
    "    \n",
    "    optimizer = torch.optim.Adam(gcn_net.parameters(), lr=0.0005)\n",
    "    \n",
    "    \n",
    "    for epoch in range(800):\n",
    "        train_accuracy,train_loss,train_auc, train_mcc,train_ba,train_f1 = train(epoch,train_loader_)\n",
    "        test_accuracy,test_loss,test_auc, test_mcc,test_ba,test_f1 = test(epoch,val_loader_val)\n",
    "\n",
    "        if test_auc > best_param[\"valid_roc\"]:\n",
    "            best_param[\"roc_epoch\"] = epoch\n",
    "            best_param[\"valid_roc\"] = test_auc\n",
    "\n",
    "        if test_mcc > best_param[\"valid_mcc\"]:\n",
    "            best_param[\"mcc_epoch\"] = epoch\n",
    "            best_param[\"valid_mcc\"] = test_mcc\n",
    "            \n",
    "        if test_mcc*0.3+test_auc*0.7 > best_param[\"valid_score\"]:\n",
    "            best_param[\"score_epoch\"] = epoch\n",
    "            best_param[\"valid_score\"] = test_mcc*0.3+test_auc*0.7\n",
    "            torch.save(gcn_net, '{}_gcn.pt'.format(i+1))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        print(\"EPOCH:\\t\"+str(epoch)+'\\n'\\\n",
    "            +\"train_roc\"+\":\"+str(train_auc)+'\\n'\\\n",
    "            +\"valid_roc\"+\":\"+str(test_auc)+'\\n'\\\n",
    "            +\"train_mcc\"+\":\"+str(train_mcc)+'\\n'\\\n",
    "            +\"valid_mcc\"+\":\"+str(test_mcc)+'\\n'\\\n",
    "            )\n",
    "        if (epoch - best_param[\"roc_epoch\"] >15) and (epoch - best_param[\"mcc_epoch\"] >20):        \n",
    "            break\n",
    "\n",
    "    gcn_net = torch.load('{}_gcn.pt'.format(i+1),weights_only=False)\n",
    "\n",
    "\n",
    "    val_epoch_acc, val_epoch_loss, val_epoch_auc,val_epoch_mcc,val_epoch_ba,val_epoch_f1 = eval(val_loader_val)\n",
    "    #test_epoch_acc, test_epoch_loss, test_epoch_auc,test_epoch_mcc,test_epoch_ba,test_epoch_f1 = eval(test_loader_test)\n",
    "\n",
    "\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_acc.append(val_epoch_acc)\n",
    "    val_auc.append(val_epoch_auc)\n",
    "    val_mcc.append(val_epoch_mcc)\n",
    "    val_ba.append(val_epoch_ba)\n",
    "    val_f1.append(val_epoch_f1)\n",
    "    #test_losses1.append(test_epoch_loss)\n",
    "    #test_acc1.append(test_epoch_acc)\n",
    "    #test_auc1.append(test_epoch_auc)\n",
    "    #test_mcc1.append(test_epoch_mcc)\n",
    "    #test_ba1.append(test_epoch_ba)\n",
    "    #test_f11.append(test_epoch_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('val_losses:',np.mean(val_losses),'+/-',np.std(val_losses))\n",
    "print('val_acc:',np.mean(val_acc),'+/-',np.std(val_acc))\n",
    "print('val_auc:',np.mean(val_auc),'+/-',np.std(val_auc))\n",
    "print('val_mcc:',np.mean(val_mcc),'+/-',np.std(val_mcc))\n",
    "print('val_ba:',np.mean(val_ba),'+/-',np.std(val_ba))\n",
    "print('val_f1:',np.mean(val_f1),'+/-',np.std(val_f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wzx)",
   "language": "python",
   "name": "wzx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
